# -*- coding: utf-8 -*-
"""loan_model (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qEGVUK2RtJXND_OQIYGTzIUjCYptV167
"""

#Importing all the libraries

import tensorflow as tf
from tensorflow import keras
import numpy as np
from tensorflow.keras.layers import Dense
import random
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

#Installing the Bias Detection Tool
!pip install aif360

#Loading the Dataset as a DataFrame
dataset = pd.read_csv('/content/loan_access_dataset.csv')

dataset.head(25)

#Checking the length of the dataset
len(dataset)

#Looking for Incomplete Data(Thankfully there werent any)
print(dataset.isnull().sum())

"""ENCODING THE DATASET"""

#Encoding all the columns for training the model
from sklearn.preprocessing import LabelEncoder
columns_to_encode = ['Race', 'Age_Group', 'Employment_Type', 'Education_Level', 'Citizenship_Status', 'Zip_Code_Group', 'Language_Proficiency', 'Disability_Status', 'Criminal_Record', 'Gender', 'Loan_Approved']
for column in columns_to_encode:
  le = LabelEncoder()
  dataset[column] = le.fit_transform(dataset[column])

for i in range(len(dataset)-1):
    if dataset['Loan_Approved'][i] == 0:
      dataset['Loan_Approved'][i] = 1
    else:
      dataset['Loan_Approved'][i] = 0

#Visualising the encoded dataset
dataset.head(25)

"""FEATURE ENGINEERING"""

#Creating a New Feature(Income_group) to evaluate financial biases
#0-Low, 1-Medium, 2-High
def income_group(income):
    if income < 60000:
        return 0
    elif income < 120000:
        return 1
    else:
        return 2

dataset['Income_Group'] = dataset['Income'].apply(income_group)

# Remove the column and save it temporarily
income_group_col = dataset.pop('Income_Group')

# Re-insert it at the desired index
dataset.insert(loc=dataset.columns.get_loc('Income') + 1, column='Income_Group', value=income_group_col)

#High - 2, Medium - 1, Low - 0

def loan_group(amt):
    if amt <= 135000:
        return 0
    elif amt <= 310000:
        return 1
    else:
        return 2

dataset['Loan_Amount_Group'] = dataset['Loan_Amount'].apply(loan_group)

# Remove the column and save it temporarily
loan_amount_col = dataset.pop('Loan_Amount_Group')

# Re-insert it at the desired index
dataset.insert(loc=dataset.columns.get_loc('Loan_Amount') + 1, column='Loan_Amount_Group', value=loan_amount_col)

# 0-Excellent, 1-Very Good, 2-Good, 3-Fair, 4-Poor
def credit_score_category(c_score):
  if c_score >= 800:
    return 0
  elif c_score >=740:
    return 1
  elif c_score >= 670:
    return 2
  elif c_score >= 580:
    return 3
  else:
    return 4

dataset['Credit_Score_Group'] = dataset['Credit_Score'].apply(credit_score_category)

# Remove the column and save it temporarily
income_group_col = dataset.pop('Credit_Score_Group')

# Re-insert it at the desired index
dataset.insert(loc=dataset.columns.get_loc('Credit_Score') + 1, column='Credit_Score_Group', value=income_group_col)

dataset.head()

"""SPLITTING THE DATASET INTO TRAIN,TEST,VALID AND THE TRAINING THE MODELS"""

#Creating Input/Output datas
X = dataset.drop('Loan_Approved', axis=1)
y = dataset['Loan_Approved']

#Importing the tool used to spilt datasets into train, test, valid
from sklearn.model_selection import train_test_split

#Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

#Importing all the Models
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost import XGBClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report, f1_score

#Training all the models and evaluating their performance
model1 = LogisticRegression(max_iter=1000)
model1.fit(X_train, y_train)
preds1 = model1.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, preds1)}, Precision: {precision_score(y_test, preds1)}, Recall: {recall_score(y_test, preds1)}")

model2 = RandomForestClassifier(n_estimators=1000)
model2.fit(X_train, y_train)
preds2 = model2.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, preds2)}, Precision: {precision_score(y_test, preds2)}, Recall: {recall_score(y_test, preds2)}")

model3 = XGBClassifier()
model3.fit(X_train, y_train)
preds3 = model3.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, preds3)}, Precision: {precision_score(y_test, preds3)}, Recall: {recall_score(y_test, preds3)}")

#Logistic Regression Model seems to have the best accuracy

"""CONFUSION MATRIX"""

#Plotting the confusion matrix for initial observations
confusion_matrix_A = confusion_matrix(y_test, preds1)
labels = ['Approved', 'Not Approved']
sns.heatmap(confusion_matrix_A, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.figsize = (6, 4)

plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

#There are 263 Cases of False Negatives(Positive bias)
#There are 467 Cases of False Positives(Negative bias)

"""TRAINING A NEURAL NETWORK TO CHECK IF ITS BETTER"""

#Training a custom neural network(Not optimal results so we will be proceeding with RandomForestClassifier)

model = tf.keras.Sequential([
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
])

model.compile(loss=tf.keras.losses.BinaryCrossentropy(),
              optimizer = tf.keras.optimizers.Adam(learning_rate=0.001),
              metrics=['accuracy'])

model.fit(X_train, y_train, epochs=50, validation_data=(X_test, y_test))

"""IBM AI FAIRNESS 360 TOOL FOR BIAS DETECTION

EVALUATION METRICS AND THEIR IDEAL RANGES:

1.  Equal Oppurtunity Difference : Checks if True positive rates are equal across groups(-0.1 to 0.1)
2.  Average Odds Difference : Combines TPR and FPR differences between groups(-0.1 to 0.1)
3. Statistical Parity Difference : Difference in favourable outcomes between groups(-0.1 to 0.1)
4. Disparate Impact : Ratio of outcomes between groups(0.8 to 1.25)
5. Theil Index : Measures inequality in predicted outcomes(-0.1 to 0.1)

Gender Bias Result
"""

#Using The IBM AI Fairness 360 Tool to check for Disparate Impact, Statistical Parity difference, Equal Oppurtunity Difference, Average Odds, Difference, Theil Index
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric

dataset2 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Gender']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Gender']
)

classified_metric = ClassificationMetric(
    dataset2, dataset_pred,
    privileged_groups=[{'Gender': 1}],
    unprivileged_groups=[{'Gender': 0}, {'Gender':2}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset2, privileged_groups=[{'Gender': 1}], unprivileged_groups=[{'Gender': 0}, {'Gender': 2}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

"""Language Proficiency Bias Result"""

#Using The IBM AI Fairness 360 Tool to check for Disparate Impact, Statistical Parity difference, Equal Oppurtunity Difference, Average Odds, Difference, Theil Index
dataset3 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Language_Proficiency']) #dataset_true
predicted_df = dataset.copy()

predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Language_Proficiency']
)

classified_metric = ClassificationMetric(
    dataset3, dataset_pred,
    privileged_groups=[{'Language_Proficiency': 0}],
    unprivileged_groups=[{'Language_Proficiency': 1}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset3, privileged_groups=[{'Language_Proficiency': 1}], unprivileged_groups=[{'Language_Proficiency': 0}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

"""Racial Bias Result"""

#Using The IBM AI Fairness 360 Tool to check for Disparate Impact, Statistical Parity difference, Equal Oppurtunity Difference, Average Odds, Difference, Theil Index
dataset4 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Race']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Race']
)

classified_metric = ClassificationMetric(
    dataset4, dataset_pred,
    privileged_groups=[{'Race': 0},{'Race':5}],
    unprivileged_groups=[{'Race': 1}, {'Race':2}, {'Race':3}, {'Race':4}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset4, privileged_groups=[{'Race': 0},{'Race':5}], unprivileged_groups=[{'Race': 1}, {'Race':2}, {'Race':3}, {'Race':4}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

"""Criminal Record Bias Result"""

#Using The IBM AI Fairness 360 Tool to check for Disparate Impact, Statistical Parity difference, Equal Oppurtunity Difference, Average Odds, Difference, Theil Index
dataset5 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Criminal_Record']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Criminal_Record']
)

classified_metric = ClassificationMetric(
    dataset5, dataset_pred,
    privileged_groups=[{'Criminal_Record': 0}],
    unprivileged_groups=[{'Criminal_Record': 1}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset5, privileged_groups=[{'Criminal_Record': 0}], unprivileged_groups=[{'Criminal_Record': 1}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

"""Disability Status Bias Result"""

dataset5 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Disability_Status']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Disability_Status']
)

classified_metric = ClassificationMetric(
    dataset5, dataset_pred,
    privileged_groups=[{'Disability_Status': 0}],
    unprivileged_groups=[{'Disability_Status': 1}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset5, privileged_groups=[{'Disability_Status': 0}], unprivileged_groups=[{'Disability_Status': 1}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

"""Employment Type Bias Result"""

dataset6 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Employment_Type']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Employment_Type']
)

classified_metric = ClassificationMetric(
    dataset6, dataset_pred,
    privileged_groups=[{'Employment_Type': 0}],
    unprivileged_groups=[{'Employment_Type': 1},{'Employment_Type': 2},{'Employment_Type': 3},{'Employment_Type': 4}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset6, privileged_groups=[{'Employment_Type': 0}], unprivileged_groups=[{'Employment_Type': 1},{'Employment_Type': 2},{'Employment_Type': 3},{'Employment_Type': 4}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

"""Age Group Bias Result"""

dataset7 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Age_Group']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Age_Group']
)

classified_metric = ClassificationMetric(
    dataset7, dataset_pred,
    privileged_groups=[{'Age_Group': 0}],
    unprivileged_groups=[{'Age_Group': 1},{'Age_Group': 2}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset7, privileged_groups=[{'Age_Group': 0}], unprivileged_groups=[{'Age_Group': 1},{'Age_Group': 2}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

"""Education Level Bias Result"""

dataset8 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Education_Level']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Education_Level']
)

classified_metric = ClassificationMetric(
    dataset8, dataset_pred,
    privileged_groups=[{'Education_Level': 0}, {'Education_Level': 1}],
    unprivileged_groups=[{'Education_Level': 2},{'Education_Level': 3}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset8, privileged_groups=[{'Education_Level': 0}, {'Education_Level': 1}], unprivileged_groups=[{'Education_Level': 2},{'Education_Level': 3}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

"""Citizenship Status Bias Result"""

dataset9 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Citizenship_Status']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Citizenship_Status']
)

classified_metric = ClassificationMetric(
    dataset9, dataset_pred,
    privileged_groups=[{'Citizenship_Status': 0}],
    unprivileged_groups=[{'Citizenship_Status': 1},{'Citizenship_Status': 2}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset9, privileged_groups=[{'Citizenship_Status': 0}], unprivileged_groups=[{'Citizenship_Status': 1},{'Citizenship_Status': 2}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

"""Zip Code Group Bias Result"""

dataset10 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Zip_Code_Group']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Zip_Code_Group']
)

classified_metric = ClassificationMetric(
    dataset10, dataset_pred,
    privileged_groups=[{'Zip_Code_Group': 0},{'Zip_Code_Group': 3}],
    unprivileged_groups=[{'Zip_Code_Group': 1},{'Zip_Code_Group': 2},{'Zip_Code_Group': 4}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset10, privileged_groups=[{'Zip_Code_Group': 0},{'Zip_Code_Group': 3}], unprivileged_groups=[{'Zip_Code_Group': 1},{'Zip_Code_Group': 2},{'Zip_Code_Group': 4}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

"""Credit Score Group Bias Result"""

# Bias between Low and High Credit people

dataset11 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Credit_Score_Group']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Credit_Score_Group']
)

classified_metric = ClassificationMetric(
    dataset11, dataset_pred,
    privileged_groups=[{'Credit_Score_Group': 0},{'Credit_Score_Group': 1}],
    unprivileged_groups=[{'Credit_Score_Group': 3},{'Credit_Score_Group': 4}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset11, privileged_groups=[{'Credit_Score_Group': 0},{'Credit_Score_Group': 1}], unprivileged_groups=[{'Credit_Score_Group': 3},{'Credit_Score_Group': 4}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

#This result clearly shows that there is a bias toward people with higher credit scores

"""Loan Amount Group Bias Result"""

#Bias between High and Low Loan Amounts
dataset12 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Loan_Amount_Group']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Loan_Amount_Group']
)

classified_metric = ClassificationMetric(
    dataset12, dataset_pred,
    privileged_groups=[{'Loan_Amount_Group': 0}],
    unprivileged_groups=[{'Loan_Amount_Group': 2}, {'Loan_Amount_Group': 1}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset12, privileged_groups=[{'Loan_Amount_Group': 0}], unprivileged_groups=[{'Loan_Amount_Group': 1},{'Loan_Amount_Group': 2}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

#This result shows that there isnt any bias regarding the Loan Amount

"""Income Group Bias Result"""

#Bias between High and Low Income groups

dataset13 = BinaryLabelDataset(df=dataset, label_names=['Loan_Approved'], protected_attribute_names=['Income_Group']) #dataset_true
predicted_df = dataset.copy()


predicted_df['Loan_Approved'] = (model1.predict_proba(X)[:, 1] >= 0.5).astype(int)

dataset_pred = BinaryLabelDataset(
    df=predicted_df,
    label_names=['Loan_Approved'],
    protected_attribute_names=['Income_Group']
)

classified_metric = ClassificationMetric(
    dataset13, dataset_pred,
    privileged_groups=[{'Income_Group': 2}],
    unprivileged_groups=[{'Income_Group': 0}, {'Income_Group': 1}]
)

# Metric for fairness
metric = BinaryLabelDatasetMetric(dataset13, privileged_groups=[{'Income_Group': 2}], unprivileged_groups=[{'Income_Group': 1},{'Income_Group': 0}])

print("Equal Opportunity Difference:", classified_metric.equal_opportunity_difference())
print("Average Odds Difference:", classified_metric.average_odds_difference())
print("Theil Index:", classified_metric.theil_index())
print("Disparate Impact:", metric.disparate_impact())
print("Statistical Parity Difference:", metric.statistical_parity_difference())

data = {
    'SPD': [0.0575, 0.0735, -0.028, -0.112, -0.214, -0.0454, -0.065, -0.032, 0.05, 0.0472, -0.093, -0.2127, -0.034],
    'DI': [0.875, 0.8389, 0.936, 0.7768, 0.6135, 0.9028, 0.8575, 0.928, 0.886, 1.121, 0.7893, 0.6995, 0.924],
    'EOD': [-0.068, -0.0464, -0.0563, -0.3614, -0.6427, -0.0905, -0.1873, -0.1911, -0.073, 0.048, -0.088, -0.0782, -0.0363],
    'AOD': [-0.0414, -0.0381, -0.042, -0.353, -0.592, -0.0868, -0.1641, 0.0086, -0.041, -0.0352, -0.0577, -0.0588, 0.0311],
    'TI': [0.3214, 0.3214, 0.3214, 0.3214, 0.3214, 0.3214, 0.3214, 0.3214, 0.3214, 0.3214, 0.3214, 0.3214, 0.3214]
}
df = pd.DataFrame(data, index=['Gender', 'Race', 'Age_Group', 'Income_Group', 'Credit_Score_Group', 'Loan_Amount_Group', 'Employment_Type', 'Education_Level', 'Citizenship_Status', 'Language_Proficiency', 'Disability_Status', 'Criminal_Record', 'Zip_Code_Group', ])

sns.heatmap(df, annot=True, cmap='coolwarm', center=0)
plt.title("AIF360 Fairness Metrics Heatmap")
plt.savefig("aif360_metrics_heatmap.png", dpi=300)
plt.show()

"""SO IN CONCLUSION,
THERE IS A STRONG BIAS TOWARD PEOPLE WITH:
1. NO CRIMINAL RECORD
2. HIGH INCOME
3. HIGH CREDIT SCORE
4. PEOPLE WITH NO DISABILITIES

MILD TO MODERATE BIAS TOWARD:
1. PEOPLE LIVING IN HIGH-INCOME SUBURBAN AND URBAN PROFESSIONAL AREAS
2. PEOPLE WITH SMALL LOAN AMOUNTS
3. PEOPLE WITH FULL TIME JOBS
4. GRADUATES AND BACHELORS

SINCE DATA IMBALANCES CAN SIGNIFICANTLY AFFECT THESE RESULTS, WE CAN CONFIRM BIAS ONLY IN THE GROUPS WHICH HAVE CONSISTENT DATA IN THIS LIST
"""

#Checking for imbalanced data
columns = ['Gender', 'Race', 'Income_Group', 'Credit_Score_Group',
           'Loan_Amount_Group', 'Employment_Type', 'Education_Level',
           'Citizenship_Status','Disability_Status',
           'Criminal_Record']

for col in columns:
  print(dataset[col].value_counts())

"""Group-wise Approval Rates"""

columns = ['Gender', 'Race', 'Age_Group', 'Income_Group', 'Credit_Score_Group',
           'Loan_Amount_Group', 'Employment_Type', 'Education_Level',
           'Citizenship_Status', 'Language_Proficiency', 'Disability_Status',
           'Criminal_Record', 'Zip_Code_Group']

for col in columns:
    print(f"\n--- {col} ---")
    approval_rates = dataset.groupby(col)['Loan_Approved'].mean()
    print(approval_rates)

"""ACCORDING TO THESE RESULTS:

1. Gender: Males have the highest approval rates while Non-Binary people have the lowest
2. Race: White People and Asians have the highest approval rates
3. Age Group: People who are 25-60 years old have a higher approval rate but by a small margin.(Not multiracial(3) as they have much lower value count).
4. Income Group: High Income people have a much higher approval rate showing a clear bias.
5. Credit Score group: High Credit score group have a much higher approval rate showing a clear bias.
6. Loan Amount: Higher Loan Amounts have a lower approval rate but by a small margin
7. Employment Type: Full-time workers have the highest approval rates followed by Gig workers and Part time workers, which indicates that there is a bias against self-employed and unemployed people.
8. Citizenship Status: Citizens have a much higher approval rate compared to visa-holders and permanent residents
9. Language Proficiency: People who are fluent have a much higher approval rate.
10. Disability Status: People with disabilities have a much lower approval rate showing a clear bias.
11. Criminal Record: People with a criminal record have a much lower approval rate showing a clear bias.
12. Zip Code Group: People from High-Income suburbs, Urban professional and rural have a higher approval rate and historically redlined areas have a significantly lower approval rate.
"""

#Visualizing the approval rates
for col in columns:
    ar = dataset.groupby(col)['Loan_Approved'].mean()
    ar.plot(kind='bar', title=f'Loan Approval Rate by {col}', ylabel='Approval Rate')
    plt.savefig(f'Approval_rates_{col}.png', dpi=300)
    plt.show()

"""**Intersection Fairness**

BASED ON RESEARCH WE ARE MOST LIKELY TO FIND BIAS IN THE FOLLOWING COMBINATIONS:
1. RACE+GENDER
2. RACE+INCOME_GROUP
3. GENDER+EMPLOYMENT TYPE
4. RACE+ZIP CODE GROUP
5. CITIZENSHIP STATUS + EMPLOYMENT TYPE
6. DISABILITY STATUS + INCOME GROUP
7. AGE GROUP + CREDIT SCORE GROUP

SO WE WILL BE VISUALOSING THESE ONE BY ONE USING SEABORN
"""

#RACE+GENDER
grouped_data = dataset.groupby(['Race', 'Gender'])['Loan_Approved'].mean().reset_index()
pivot = grouped_data.pivot(index='Race', columns='Gender', values='Loan_Approved')
sns.heatmap(pivot, annot=True, cmap='coolwarm')
plt.title("Approval Rates by Race & Gender")
plt.show()

#RACE+ INCOME GROUP
grouped_data = dataset.groupby(['Race', 'Income_Group'])['Loan_Approved'].mean().reset_index()
pivot = grouped_data.pivot(index='Race', columns='Income_Group', values='Loan_Approved')
sns.heatmap(pivot, annot=True, cmap='coolwarm')
plt.title("Approval Rates by Race & Income Group")
plt.show()

#GENDER + EMPLOYMENT TYPE
grouped_data = dataset.groupby(['Gender', 'Employment_Type'])['Loan_Approved'].mean().reset_index()
pivot = grouped_data.pivot(index='Gender', columns='Employment_Type', values='Loan_Approved')
sns.heatmap(pivot, annot=True, cmap='coolwarm')
plt.title("Approval Rates by Gender & Employment_Type")
plt.show()

#RACE + ZIP CODE GROUP
grouped_data = dataset.groupby(['Race', 'Zip_Code_Group'])['Loan_Approved'].mean().reset_index()
pivot = grouped_data.pivot(index='Race', columns='Zip_Code_Group', values='Loan_Approved')
sns.heatmap(pivot, annot=True, cmap='coolwarm')
plt.title("Approval Rates by Race & Zip_Code_Group")
plt.show()

#CITIZENSHIP STATUS + EMPLOYMENT TYPE
grouped_data = dataset.groupby(['Citizenship_Status', 'Employment_Type'])['Loan_Approved'].mean().reset_index()
pivot = grouped_data.pivot(index='Citizenship_Status', columns='Employment_Type', values='Loan_Approved')
sns.heatmap(pivot, annot=True, cmap='coolwarm')
plt.title("Approval Rates by Citizenship_Status & Employment_Type")
plt.show()

#DISABILITY STATUS + INCOME GROUP
grouped_data = dataset.groupby(['Disability_Status', 'Income_Group'])['Loan_Approved'].mean().reset_index()
pivot = grouped_data.pivot(index='Disability_Status', columns='Income_Group', values='Loan_Approved')
sns.heatmap(pivot, annot=True, cmap='coolwarm')
plt.title("Approval Rates by Disability_Status & Income_Group")
plt.show()

#AGE GROUP + CREDIT SCORE GROUP
grouped_data = dataset.groupby(['Age_Group', 'Credit_Score_Group'])['Loan_Approved'].mean().reset_index()
pivot = grouped_data.pivot(index='Age_Group', columns='Credit_Score_Group', values='Loan_Approved')
sns.heatmap(pivot, annot=True, cmap='coolwarm')
plt.title("Approval Rates by Age_Group & Credit_Score_Group")
plt.show()

"""FROM THESE INTERSECTION FAIRNESS HEATMAPS WE CAN CONCLUDE THAT THE GROUPS WHICH ARE BEING FAVOURED ARE:
1. Citizens with top-tier jobs get best outcomes.
2. Older People with best credit get highest approvals.
3. Disabled + low income = highest approval. Suggests support policy.
4. Non-Binary people with mid employment — best treated group.
5. Black + Non-Binary = highest approval.
6. Native American + low income = highest approval — possible bias or priority policy.

Visual Explanation(SHAP, LIME)
"""

!pip install shap

"""WHAT THE BELOW CODE HELPS US VISUALIZE

1. Bar: Overall feature importance
2. Force Plot: Explaining a single decision
3. Beeswarm(Summary Plot): Global feature influence
4. Scatter: plots all the above shap features on the graph
"""

#SHAP values for all the columns
import shap
import matplotlib.pyplot as plt
columns = ['Gender', 'Race', 'Age_Group', 'Income_Group', 'Credit_Score_Group',
           'Loan_Amount_Group', 'Employment_Type', 'Education_Level',
           'Citizenship_Status', 'Language_Proficiency', 'Disability_Status',
           'Criminal_Record', 'Zip_Code_Group']

X_sample = X_test.sample(100, random_state=42)
explainer = shap.Explainer(model1, X_sample)
shap_values = explainer(X_sample)

shap.plots.bar(shap_values)
shap.plots.beeswarm(shap_values)
shap.initjs()
shap.force_plot(
    base_value=shap_values.base_values[0],
    shap_values=shap_values.values[0],
    features=X_test.iloc[0],
    feature_names=X_test.columns
)
for col in columns:
  shap.plots.scatter(shap_values[:, col], color=shap_values)

!pip install lime

from lime.lime_tabular import LimeTabularExplainer

explainer = LimeTabularExplainer(training_data = X_train.values, feature_names=X_train.columns, class_names=['Not Approved', 'Approved'], mode='classification')

i=5
exp = explainer.explain_instance(data_row=X_test.iloc[i].values,predict_fn=model1.predict_proba)
exp.show_in_notebook(show_table=True)

"""From the above plots we can conclude:
Likely positive influences (red):
  1. High income

  2. No criminal record

  3. High language proficiency

  4. High income group

Likely negative influences (blue):
  1. High loan amount

  2. Possibly young age

  3. Possibly Gender, depending on model bias

MODEL PREDICTIONS ON TEST DATASET
"""

test_dataset = pd.read_csv('/content/test.csv')
test_dataset.head()

#Encoding all the columns for training the model
from sklearn.preprocessing import LabelEncoder
columns_to_encode = ['Race', 'Age_Group', 'Employment_Type', 'Education_Level', 'Citizenship_Status', 'Zip_Code_Group', 'Language_Proficiency', 'Disability_Status', 'Criminal_Record', 'Gender']
for column in columns_to_encode:
  le = LabelEncoder()
  test_dataset[column] = le.fit_transform(test_dataset[column])

#Creating a New Feature(Income_group) to evaluate financial biases
#0-Low, 1-Medium, 2-High
def income_group(income):
    if income < 60000:
        return 0
    elif income < 120000:
        return 1
    else:
        return 2

test_dataset['Income_Group'] = test_dataset['Income'].apply(income_group)

# Remove the column and save it temporarily
income_group_col = test_dataset.pop('Income_Group')

# Re-insert it at the desired index
test_dataset.insert(loc=dataset.columns.get_loc('Income') + 1, column='Income_Group', value=income_group_col)

#High - 2, Medium - 1, Low - 0

def loan_group(amt):
    if amt <= 135000:
        return 0
    elif amt <= 310000:
        return 1
    else:
        return 2

test_dataset['Loan_Amount_Group'] = test_dataset['Loan_Amount'].apply(loan_group)

# 0-Excellent, 1-Very Good, 2-Good, 3-Fair, 4-Poor
def credit_score_category(c_score):
  if c_score >= 800:
    return 0
  elif c_score >=740:
    return 1
  elif c_score >= 670:
    return 2
  elif c_score >= 580:
    return 3
  else:
    return 4

test_dataset['Credit_Score_Group'] = test_dataset['Credit_Score'].apply(credit_score_category)

new_order = ['ID', 'Gender', 'Race', 'Age', 'Age_Group', 'Income', 'Income_Group', 'Credit_Score', 'Credit_Score_Group', 'Loan_Amount', 'Loan_Amount_Group', 'Employment_Type', 'Education_Level', 'Citizenship_Status', 'Language_Proficiency', 'Disability_Status', 'Criminal_Record', 'Zip_Code_Group']
test_dataset_reordered = test_dataset[new_order]

test_dataset_reordered.head()

test_preds = model1.predict(test_dataset_reordered)

test_preds

test_dataset_reordered['Loan_Approved'] = test_preds

test_dataset_reordered.head()

submission = test_dataset_reordered[['ID', 'Loan_Approved']]
submission.head(25)

submission.to_csv("/content/submission.csv", index=False)

from google.colab import files
files.download("submission.csv")

